# Real-Time Fraud Detection System

A production-grade real-time fraud detection system using PySpark, AWS Kinesis, SageMaker, and EKS.

## ğŸ—ï¸ Architecture Overview

```
Synthetic Data â†’ AWS Kinesis â†’ PySpark Streaming â†’ ML Model (SageMaker) â†’ DynamoDB (Alerts) + S3 (Raw Data)
```

## ğŸ“‹ Project Status

**Current Sprint:** Sprint 1 - Data Producer & Local Development Setup

### Sprint Breakdown
- âœ… **Sprint 1:** Synthetic Data Producer + Kinesis Integration
- â³ **Sprint 2:** PySpark Structured Streaming Consumer
- â³ **Sprint 3:** ML Model Integration (SageMaker)
- â³ **Sprint 4:** Sinks (DynamoDB + S3)
- â³ **Sprint 5:** EKS Deployment + Infrastructure as Code

## ğŸš€ Quick Start (Sprint 1)

### Prerequisites
- Python 3.9+
- Docker & Docker Compose
- AWS CLI (optional, for real AWS)

### Local Development with LocalStack

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Start LocalStack (simulates AWS locally):**
   ```bash
   docker-compose up -d
   ```

3. **Create Kinesis stream:**
   ```bash
   python scripts/setup_localstack.py
   ```

4. **Run the producer:**
   ```bash
   python -m producer.main --num-transactions 100 --batch-size 10
   ```

### Using Real AWS (Alternative)

1. **Configure AWS credentials:**
   ```bash
   aws configure
   ```

2. **Create Kinesis stream:**
   ```bash
   aws kinesis create-stream --stream-name fraud-transactions --shard-count 1
   ```

3. **Run producer with AWS:**
   ```bash
   python -m producer.main --use-aws --stream-name fraud-transactions
   ```

## ğŸ“ Project Structure

```
realtime-fraud-pyspark-eks-sagemaker/
â”œâ”€â”€ producer/                    # Sprint 1: Data generation & Kinesis producer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # Entry point
â”‚   â”œâ”€â”€ data_generator.py       # Synthetic transaction generator
â”‚   â”œâ”€â”€ kinesis_producer.py     # Kinesis client wrapper
â”‚   â””â”€â”€ config.py               # Configuration management
â”œâ”€â”€ processor/                   # Sprint 2: PySpark streaming job
â”‚   â””â”€â”€ (TBD)
â”œâ”€â”€ ml_model/                    # Sprint 3: ML model integration
â”‚   â””â”€â”€ (TBD)
â”œâ”€â”€ infrastructure/              # Sprint 5: Terraform/K8s configs
â”‚   â””â”€â”€ (TBD)
â”œâ”€â”€ tests/                       # Unit & integration tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_producer.py
â”œâ”€â”€ scripts/                     # Utility scripts
â”‚   â””â”€â”€ setup_localstack.py
â”œâ”€â”€ config/                      # Configuration files
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ docker-compose.yml          # LocalStack setup
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ pyproject.toml             # Project metadata
â””â”€â”€ README.md
```

## ğŸ¯ Sprint 1 Learning Objectives

1. **Production-grade code structure:** Separation of concerns, modularity
2. **Type hints:** Using Python typing for better code quality
3. **Configuration management:** Externalized config using YAML
4. **AWS SDK:** Working with boto3 and Kinesis
5. **Local development:** Using LocalStack to avoid AWS costs
6. **Error handling & logging:** Proper exception handling and observability

## ğŸ“š Next Steps

After completing Sprint 1, you'll have:
- âœ… A working synthetic data producer
- âœ… Data flowing to Kinesis (local or AWS)
- âœ… Foundation for the streaming processor

Move to Sprint 2 to build the PySpark consumer!
